model_name: google/gemma-3-1b-it
max_seq_len: 2048
seed: 4567
lora:
  r: 8
  alpha: 32
  dropout: 0
train:
  per_device_train_batch_size: 8
  gradient_accumulation_steps: 1
  warmup_ratio: 0.1
  epochs: 1
  learning_rate: 1e-4
  lr_scheduler_type: "linear"
  optim: "adamw_8bit"
  weight_decay: 0.001
  logging_steps: 10
path:
  output_dir: "outputs/gem3wsp-1b-lora"